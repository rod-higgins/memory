"""
Personal SLM - The trained model for memory-based reasoning.

This is the core model that has been fine-tuned on personal memories.
It provides personalized context and reasoning to enhance interactions
with larger LLMs.
"""

from __future__ import annotations

from abc import ABC, abstractmethod
from dataclasses import dataclass

import httpx


@dataclass
class PersonalContext:
    """Context generated by the Personal SLM."""

    # The generated personalized context
    context: str

    # Reasoning about how this context applies
    reasoning: str | None = None

    # Confidence in the context's relevance
    confidence: float = 0.8

    # Which memory domains were activated
    activated_domains: list[str] | None = None

    # Processing time in milliseconds
    processing_time_ms: float = 0.0


class PersonalSLM(ABC):
    """
    Abstract base class for Personal SLM.

    The Personal SLM is a fine-tuned language model that has internalized
    the user's knowledge, preferences, and context. It provides:

    1. Personalized context generation
    2. Query understanding through user's lens
    3. Memory-enhanced reasoning
    4. Integration with larger LLMs
    """

    @abstractmethod
    async def generate_context(
        self,
        query: str,
        max_tokens: int = 500,
    ) -> PersonalContext:
        """
        Generate personalized context for a query.

        This is the primary method - takes a user query and generates
        relevant personalized context based on learned memories.
        """
        pass

    @abstractmethod
    async def enhance_prompt(
        self,
        user_prompt: str,
        system_prompt: str | None = None,
    ) -> str:
        """
        Enhance a prompt with personalized context.

        Takes a user's prompt and returns an enhanced version
        with relevant personal context injected.
        """
        pass

    @abstractmethod
    async def is_available(self) -> bool:
        """Check if the model is available for inference."""
        pass


class OllamaPersonalSLM(PersonalSLM):
    """
    Personal SLM running via Ollama.

    Uses a fine-tuned model hosted in Ollama for inference.
    """

    def __init__(
        self,
        model_name: str = "personal-slm",
        base_url: str = "http://localhost:11434",
        fallback_model: str = "llama3.2:3b",
    ):
        self._model_name = model_name
        self._base_url = base_url
        self._fallback_model = fallback_model

    async def generate_context(
        self,
        query: str,
        max_tokens: int = 500,
    ) -> PersonalContext:
        """Generate personalized context for a query."""
        import time

        start = time.time()

        prompt = f"""Based on your understanding of the user, provide relevant context for this query.
Focus on their preferences, past experiences, and relevant knowledge.

Query: {query}

Provide context that would help another AI assistant give a more personalized response.
Be specific and relevant. If you don't have relevant context, say so briefly.

Personal Context:"""

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self._base_url}/api/generate",
                    json={
                        "model": self._model_name,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "num_predict": max_tokens,
                            "temperature": 0.7,
                        },
                    },
                    timeout=30.0,
                )
                response.raise_for_status()
                context = response.json()["response"]

                elapsed = (time.time() - start) * 1000

                return PersonalContext(
                    context=context.strip(),
                    confidence=0.8,
                    processing_time_ms=elapsed,
                )

        except httpx.HTTPError:
            # Try fallback model
            return await self._generate_with_fallback(query, max_tokens, start)

    async def _generate_with_fallback(
        self,
        query: str,
        max_tokens: int,
        start_time: float,
    ) -> PersonalContext:
        """Use fallback model if personal SLM isn't available."""
        import time

        prompt = f"""You are helping personalize a response. Consider what context might be relevant.

Query: {query}

What personal context might be helpful here? (Keep it brief and relevant)

Context:"""

        try:
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self._base_url}/api/generate",
                    json={
                        "model": self._fallback_model,
                        "prompt": prompt,
                        "stream": False,
                        "options": {
                            "num_predict": max_tokens,
                            "temperature": 0.7,
                        },
                    },
                    timeout=30.0,
                )
                response.raise_for_status()
                context = response.json()["response"]

                elapsed = (time.time() - start_time) * 1000

                return PersonalContext(
                    context=context.strip(),
                    confidence=0.5,  # Lower confidence for fallback
                    processing_time_ms=elapsed,
                )

        except httpx.HTTPError:
            elapsed = (time.time() - start_time) * 1000
            return PersonalContext(
                context="",
                confidence=0.0,
                processing_time_ms=elapsed,
            )

    async def enhance_prompt(
        self,
        user_prompt: str,
        system_prompt: str | None = None,
    ) -> str:
        """Enhance a prompt with personalized context."""
        context = await self.generate_context(user_prompt)

        if not context.context:
            return system_prompt or ""

        enhanced = []

        if system_prompt:
            enhanced.append(system_prompt)
            enhanced.append("")

        enhanced.append("<personal_context>")
        enhanced.append(context.context)
        enhanced.append("</personal_context>")

        return "\n".join(enhanced)

    async def is_available(self) -> bool:
        """Check if Ollama and the model are available."""
        try:
            async with httpx.AsyncClient() as client:
                response = await client.get(
                    f"{self._base_url}/api/tags",
                    timeout=5.0,
                )
                response.raise_for_status()
                models = response.json().get("models", [])

                # Check if personal model or fallback is available
                model_names = [m.get("name", "") for m in models]
                return self._model_name in model_names or any(self._fallback_model in n for n in model_names)

        except httpx.HTTPError:
            return False


class HybridPersonalSLM(PersonalSLM):
    """
    Hybrid Personal SLM that combines trained model with RAG.

    Uses both:
    1. Fine-tuned model for learned representations
    2. RAG for explicit memory retrieval

    This provides the best of both approaches.
    """

    def __init__(
        self,
        slm: OllamaPersonalSLM | None = None,
        storage_path: str = "~/memory/data",
    ):
        self._slm = slm or OllamaPersonalSLM()
        self._storage_path = storage_path
        self._api = None

    async def _ensure_api(self) -> None:
        """Ensure the memory API is initialized."""
        if self._api is None:
            from memory.api.memory_api import MemoryAPI

            self._api = MemoryAPI(base_path=self._storage_path)
            await self._api.initialize()

    async def generate_context(
        self,
        query: str,
        max_tokens: int = 500,
    ) -> PersonalContext:
        """Generate context using both SLM and RAG."""
        import time

        start = time.time()

        await self._ensure_api()

        # Get RAG context
        rag_context = await self._api.get_context(query, format="compact", max_memories=5)

        # Get SLM context
        slm_context = await self._slm.generate_context(query, max_tokens=max_tokens // 2)

        # Combine
        combined_parts = []

        if slm_context.context:
            combined_parts.append("Personalized Understanding:")
            combined_parts.append(slm_context.context)

        if rag_context:
            combined_parts.append("")
            combined_parts.append("Relevant Memories:")
            combined_parts.append(rag_context)

        elapsed = (time.time() - start) * 1000

        return PersonalContext(
            context="\n".join(combined_parts),
            confidence=(slm_context.confidence + 0.8) / 2,  # Average confidence
            processing_time_ms=elapsed,
        )

    async def enhance_prompt(
        self,
        user_prompt: str,
        system_prompt: str | None = None,
    ) -> str:
        """Enhance a prompt using hybrid approach."""
        context = await self.generate_context(user_prompt)

        enhanced = []

        if system_prompt:
            enhanced.append(system_prompt)
            enhanced.append("")

        enhanced.append("<user_profile>")
        enhanced.append(context.context)
        enhanced.append("</user_profile>")

        return "\n".join(enhanced)

    async def is_available(self) -> bool:
        """Check availability."""
        await self._ensure_api()
        return True  # RAG is always available as fallback

    async def close(self) -> None:
        """Close connections."""
        if self._api:
            await self._api.close()
            self._api = None


async def get_personal_slm(
    model_name: str = "personal-slm",
    use_hybrid: bool = True,
) -> PersonalSLM:
    """
    Factory function to get a Personal SLM instance.

    Args:
        model_name: Name of the Ollama model
        use_hybrid: Whether to use hybrid (SLM + RAG) approach

    Returns:
        Configured Personal SLM
    """
    slm = OllamaPersonalSLM(model_name=model_name)

    if use_hybrid:
        return HybridPersonalSLM(slm=slm)

    return slm
